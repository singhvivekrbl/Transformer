{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = ['The', 'cat', 'sat', 'on', 'the', 'mat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {\"The\": 0, \"cat\": 1, \"sat\": 2, \"on\": 3, \"the\": 4, \"mat\": 5}\n",
    "tokenized_document = [vocab[word] for word in document]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "document = [\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n",
    "vocab = {\"The\": 0, \"cat\": 1, \"sat\": 2, \"on\": 3, \"the\": 4, \"mat\": 5}\n",
    "\n",
    "# Convert words to indices\n",
    "tokenized_document = [vocab[word] for word in document]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total vocab length:  6\n",
      "embeding dimention:  Embedding(6, 8)\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Define the embedding layer\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 8 # Dimension of the embeddings\n",
    "embedding_layer = nn.Embedding(num_embeddings=vocab_size, embedding_dim= embedding_dim)\n",
    "\n",
    "print(\"Total vocab length: \",vocab_size)\n",
    "print(\"embeding dimention: \",embedding_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert tokenized document to a tensor:\n",
    "tokenized_document_tensor=torch.tensor(tokenized_document, dtype=torch.long)\n",
    "tokenized_document_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4852, -1.2485,  0.0559,  1.3282, -0.1493, -0.1792, -0.4834,  0.8161],\n",
       "        [-0.8813, -1.0965, -0.7389,  0.8518, -0.3262,  1.4533, -0.4729,  1.5557],\n",
       "        [-0.3882, -1.1924, -1.7779,  0.5747, -1.6086, -0.8626,  0.9148,  0.4478],\n",
       "        [-0.9386, -1.1601,  1.7668, -0.4469, -0.5911, -0.9946,  0.4276,  0.2412],\n",
       "        [-0.4059, -0.5693,  0.6429, -1.1780,  1.9843,  0.5548, -0.3797,  0.0209],\n",
       "        [-0.8943, -0.1529, -0.2080, -0.0604, -1.7326,  0.1557,  0.7743,  0.5963]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 3: Pass the tokenized document through the embedding layer\n",
    "embedded_document = embedding_layer(tokenized_document_tensor)\n",
    "embedded_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Document:  [0, 1, 2, 3, 4, 5]\n",
      "Embedded Document:\n",
      " tensor([[ 0.4852, -1.2485,  0.0559,  1.3282, -0.1493, -0.1792, -0.4834,  0.8161],\n",
      "        [-0.8813, -1.0965, -0.7389,  0.8518, -0.3262,  1.4533, -0.4729,  1.5557],\n",
      "        [-0.3882, -1.1924, -1.7779,  0.5747, -1.6086, -0.8626,  0.9148,  0.4478],\n",
      "        [-0.9386, -1.1601,  1.7668, -0.4469, -0.5911, -0.9946,  0.4276,  0.2412],\n",
      "        [-0.4059, -0.5693,  0.6429, -1.1780,  1.9843,  0.5548, -0.3797,  0.0209],\n",
      "        [-0.8943, -0.1529, -0.2080, -0.0604, -1.7326,  0.1557,  0.7743,  0.5963]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Print the embbed document\n",
    "print(\"Tokenized Document: \", tokenized_document)\n",
    "print(\"Embedded Document:\\n\", embedded_document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "__1. Vocabulary Creation:__ We manually create a vocabulary that maps each unique word to an index.\n",
    "\n",
    "__2. Tokenization:__ We convert each word in the document to its corresponding index based on the vocabulary.\n",
    "\n",
    "__3. Embedding Layer:__ We define an nn.Embedding layer with the vocabulary size and the desired embedding dimension. This layer is trainable and will learn embeddings during training if integrated into a larger model.\n",
    "\n",
    "__4. Tensor Conversion:__ We convert the tokenized document into a PyTorch tensor.\n",
    "\n",
    "__5. Generate Embeddings:__ We pass the tokenized tensor through the embedding layer to obtain the embeddings.\n",
    "\n",
    "\n",
    "### Next Steps\n",
    "After generating embeddings for a document, the next steps typically involve integrating these embeddings into a more complex model like a Transformer encoder or decoder. If you want to proceed with creating other components of the Transformer model, let me know which part you'd like to tackle next!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer_embedding",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
